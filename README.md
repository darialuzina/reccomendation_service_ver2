# Recommendation Service using CatBoost & BERT Embeddings

## ðŸ“Œ Overview
This project is an enhanced machine learning-based recommendation service that processes post text, extracts advanced features using BERT embeddings, and employs CatBoost to generate more accurate recommendations. The service is optimized for speed, integrating a database for efficient storage and exposing an API via FastAPI.

### ðŸ”¥ Updates in Version 2
âœ… **BERT-based Feature Extraction** â€“ Extracts semantic meaning from post text using `all-MiniLM-L6-v2` SentenceTransformer.  
âœ… **Improved Accuracy** â€“ Model accuracy increased by approximately **0.5%** with new features.  
âœ… **Optimized Feature Engineering** â€“ Added mean, max, and min of embeddings as new features.  
âœ… **Efficient Storage** â€“ Features stored in a database for optimized speed.  
âœ… **API Service** â€“ Built with FastAPI for seamless integration.  
âœ… **Postman Integration** â€“ Easily test API endpoints.  

---

## ðŸ“‚ Project Structure
```bash
recommendation-service_ver2/      # Root directory of the project
â”‚â”€â”€ README.md                # Project documentation
â”‚â”€â”€ requirements.txt         # Dependencies list
â”‚â”€â”€ .gitignore/               # Git ignore file
â”‚â”€â”€ notebooks/               # Jupyter Notebooks for model training & feature engineering
â”‚   â”œâ”€â”€ Feature_databases.ipynb # Feature extraction & database storage
â”‚   â”œâ”€â”€ Model.ipynb             # Model training & evaluation
â”‚â”€â”€ models/                  # Pre-trained models
â”‚   â”œâ”€â”€ catboost_model.cbm      # Saved CatBoost model
â”‚â”€â”€ service/                 # API implementation using FastAPI
â”‚   â”œâ”€â”€ service.py              # API endpoints and service logic
â”‚   â”œâ”€â”€ schema.py               # API schema definitions
```

## ðŸš€ Feature Engineering
### BERT-based Vectorization of Posts & Feature Extraction**
We leverage **BERT embeddings** to capture semantic similarities in post text.

#### **What Are Embeddings?**
Embeddings are numerical representations of text that capture the semantic meaning of words and phrases. Unlike traditional methods that rely on simple word frequency, embeddings encode contextual relationships between words. This helps in:
- Identifying similar posts even when different words are used.
- Understanding context beyond keyword matching.
- Improving recommendation accuracy by capturing deeper semantic patterns in text data.

## ðŸ“Œ Future Improvements

ðŸ”¹ Further fine-tuning of BERT embeddings for domain-specific texts.
ðŸ”¹ Experimenting with additional feature engineering techniques.
ðŸ”¹ Deploying the model as a cloud-based microservice.
ðŸ”¹ Exploring deep learning-based recommendation models, such as Transformer-based architectures.
ðŸ”¹ Implementing reinforcement learning to optimize recommendations dynamically.
ðŸ”¹ Using graph neural networks (GNNs) to model relationships between users and posts.
